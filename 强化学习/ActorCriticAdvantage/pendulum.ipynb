{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor\n",
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, n_features, n_hidden, n_outputs):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_features, n_hidden)\n",
    "        self.mu = nn.Linear(n_hidden, n_outputs)\n",
    "        self.sigma = nn.Linear(n_hidden, n_outputs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        mu = self.mu(x)\n",
    "        mu = torch.tanh(mu) # [-1, 1]\n",
    "        sigma = self.sigma(x)\n",
    "        sigma = F.softplus(sigma) # [0, âˆž]\n",
    "        return mu, sigma\n",
    "\n",
    "class Actor(object):\n",
    "    def __init__(self, n_features, action_bound, n_hidden=30, lr=0.0001):\n",
    "        self.n_features = n_features\n",
    "        self.action_bound = action_bound\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "\n",
    "        self.__build_net()\n",
    "\n",
    "    def __build_net(self):\n",
    "        self.actor_net = ActorNet(self.n_features, self.n_hidden, 1)\n",
    "        self.optimizer = Adam(self.actor_net.parameters(), lr=self.lr)\n",
    "\n",
    "    def normal_dist(self, s):\n",
    "        s = torch.FloatTensor(s[np.newaxis, :])\n",
    "        mu, sigma = self.actor_net(s)\n",
    "        mu, sigma = (mu * 2).squeeze(),  (sigma + 0.1).squeeze()\n",
    "        # get the normal distribution of average=mu and std=sigma\n",
    "        normal_dist = torch.distributions.Normal(mu, sigma)\n",
    "        return normal_dist\n",
    "    \n",
    "    def choose_action(self, s):\n",
    "        self.actor_net.eval()\n",
    "        with torch.no_grad():\n",
    "            # sample action accroding to the distribution\n",
    "            normal_dist = self.normal_dist(s)\n",
    "        action = torch.clamp(normal_dist.sample(), self.action_bound[0], self.action_bound[1])\n",
    "        return action.item()\n",
    "\n",
    "    def learn(self, s, a, td):\n",
    "        self.actor_net.train()\n",
    "        normal_dist = self.normal_dist(s)\n",
    "        # log_prob get the probability of action a under the distribution of normal_dist\n",
    "        log_prob = normal_dist.log_prob(torch.tensor(a))\n",
    "        # advantage (TD_error) guided loss\n",
    "        exp_v = log_prob * torch.tensor(td.item())\n",
    "        # Add cross entropy cost to encourage exploration\n",
    "        exp_v += 0.01 * normal_dist.entropy()\n",
    "        # max(v) = min(-v)\n",
    "        loss = - exp_v   \n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return exp_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, n_features, n_hidden, n_outputs):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_features, n_hidden)\n",
    "        self.v = nn.Linear(n_hidden, n_outputs)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.v(x)\n",
    "        return x\n",
    "\n",
    "class Critic:\n",
    "    def __init__(self, n_features, n_hidden=30, n_output=1, lr=0.01, gamma=0.9):\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.__build_net()\n",
    "\n",
    "    def __build_net(self):\n",
    "        self.critic_net = CriticNet(self.n_features, self.n_hidden, self.n_output)\n",
    "        self.optimizer = Adam(self.critic_net.parameters(), lr=self.lr)\n",
    "    \n",
    "    def learn(self, s, r, s_):\n",
    "        s, s_ = torch.FloatTensor(s[np.newaxis, :]), torch.FloatTensor(s_[np.newaxis, :])\n",
    "        v, v_ = self.critic_net(s), self.critic_net(s_)\n",
    "        td_error = torch.mean(r + self.gamma * v_.double() - v.double())\n",
    "        loss = td_error ** 2\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODE = 1000\n",
    "MAX_EP_STEPS = 200\n",
    "DISPLAY_REWARD_THRESHOLD = -100\n",
    "RENDER = False\n",
    "LR_A = 0.001\n",
    "LR_C = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "env = env.unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_S = env.observation_space.shape[0]\n",
    "A_BOUND = env.action_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(n_features=N_S, lr=LR_A, action_bound=[float(-A_BOUND), float(A_BOUND)])\n",
    "critic = Critic(n_features=N_S, lr=LR_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0   reward: -142\n",
      "episode:  1   reward: -139\n",
      "episode:  2   reward: -140\n",
      "episode:  3   reward: -138\n",
      "episode:  4   reward: -139\n",
      "episode:  5   reward: -135\n",
      "episode:  6   reward: -136\n",
      "episode:  7   reward: -139\n",
      "episode:  8   reward: -136\n",
      "episode:  9   reward: -138\n",
      "episode:  10   reward: -139\n",
      "episode:  11   reward: -139\n",
      "episode:  12   reward: -139\n",
      "episode:  13   reward: -137\n",
      "episode:  14   reward: -136\n",
      "episode:  15   reward: -138\n",
      "episode:  16   reward: -140\n",
      "episode:  17   reward: -136\n",
      "episode:  18   reward: -138\n",
      "episode:  19   reward: -139\n",
      "episode:  20   reward: -139\n",
      "episode:  21   reward: -141\n",
      "episode:  22   reward: -142\n",
      "episode:  23   reward: -141\n",
      "episode:  24   reward: -142\n",
      "episode:  25   reward: -143\n",
      "episode:  26   reward: -138\n",
      "episode:  27   reward: -139\n",
      "episode:  28   reward: -140\n",
      "episode:  29   reward: -140\n",
      "episode:  30   reward: -140\n",
      "episode:  31   reward: -137\n",
      "episode:  32   reward: -139\n",
      "episode:  33   reward: -139\n",
      "episode:  34   reward: -140\n",
      "episode:  35   reward: -139\n",
      "episode:  36   reward: -138\n",
      "episode:  37   reward: -138\n",
      "episode:  38   reward: -139\n",
      "episode:  39   reward: -138\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(MAX_EPISODE):\n",
    "\ts = env.reset()\n",
    "\tt = 0\n",
    "\tep_rs = []\n",
    "\twhile True:\n",
    "\t\tif RENDER: env.render()\n",
    "\t\ta = actor.choose_action(s)\n",
    "\n",
    "\t\ts_, r, done, info = env.step([a])\n",
    "\t\tr /= 10\n",
    "\n",
    "\t\ttd_error = critic.learn(s, r, s_)   # gradient = grad[r + gamma * V(s_) - V(s)]\n",
    "\t\tactor.learn(s, a, td_error)   # gradient = grad[logPi(s, a) * td_error]\n",
    "\n",
    "\t\ts = s_\n",
    "\t\tt += 1\n",
    "\t\tep_rs.append(r)\n",
    "\t\tif t > MAX_EP_STEPS:\n",
    "\t\t\tep_rs_sum = sum(ep_rs)\n",
    "\t\t\tif 'running_reward' not in globals():\n",
    "\t\t\t\trunning_reward = ep_rs_sum\n",
    "\t\t\telse:\n",
    "\t\t\t\trunning_reward = running_reward * 0.9 + ep_rs_sum * 0.1\n",
    "\t\t\tif running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True\n",
    "\t\t\tprint('episode: ', i_episode, '  reward:', int(running_reward))\n",
    "\t\t\tbreak\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41012682a26ab7523e1c2229b30ca43de5762a175f7ae25c25f2d59b0d2e0e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
